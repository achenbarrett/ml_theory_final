{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13a35a90",
   "metadata": {},
   "source": [
    "# Topic Modeling using doc2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e4d43e",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "21d9de12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import collections\n",
    "import string\n",
    "\n",
    "import nagisa\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import utils\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2624166a",
   "metadata": {},
   "source": [
    "## Data Input and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2390b1ed",
   "metadata": {},
   "source": [
    "### Read in Wikipedia data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5c15df50",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>views</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>メインページ</td>\n",
       "      <td>362562853</td>\n",
       "      <td>ようこそ\\nウィキペディア - ウィキペディア日本語版 - 百科事典目次\\n検索資料・ポータ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>星野源</td>\n",
       "      <td>10190763</td>\n",
       "      <td>星野 源（ほしの みなもと、1981年1月28日 - ）は、日本の音楽家、俳優、文筆家。埼玉...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>真田信繁</td>\n",
       "      <td>9602104</td>\n",
       "      <td>真田 信繁（さなだ のぶしげ）は、安土桃山時代から江戸時代初期にかけての武将、大名。真田昌幸...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>高橋一生</td>\n",
       "      <td>8571666</td>\n",
       "      <td>高橋 一生（たかはし いっせい、英字表記：Issey Takahashi、1980年12月9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>君の名は。</td>\n",
       "      <td>7788879</td>\n",
       "      <td>『君の名は。』（きみのなは、英: Your Name.）は、2016年に公開された新海誠監督...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  article      views                                               text\n",
       "0  メインページ  362562853  ようこそ\\nウィキペディア - ウィキペディア日本語版 - 百科事典目次\\n検索資料・ポータ...\n",
       "1     星野源   10190763  星野 源（ほしの みなもと、1981年1月28日 - ）は、日本の音楽家、俳優、文筆家。埼玉...\n",
       "2    真田信繁    9602104  真田 信繁（さなだ のぶしげ）は、安土桃山時代から江戸時代初期にかけての武将、大名。真田昌幸...\n",
       "3    高橋一生    8571666  高橋 一生（たかはし いっせい、英字表記：Issey Takahashi、1980年12月9...\n",
       "4   君の名は。    7788879  『君の名は。』（きみのなは、英: Your Name.）は、2016年に公開された新海誠監督..."
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_df = pd.read_pickle('wiki') \n",
    "wiki_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "947455db",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(wiki_df['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44edfd41",
   "metadata": {},
   "source": [
    "### Use nagisa to generate delimited, labeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e146937e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_split = []\n",
    "for article in data:\n",
    "    article_split = re.split('\\n|\\.|!|！|。', article)\n",
    "    data_split.append(article_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c326d3c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished 0\n",
      "finished 1\n",
      "finished 2\n",
      "finished 3\n",
      "finished 4\n",
      "finished 5\n",
      "finished 6\n",
      "finished 7\n",
      "finished 8\n",
      "finished 9\n",
      "finished 10\n",
      "finished 11\n",
      "finished 12\n",
      "finished 13\n",
      "finished 14\n",
      "finished 15\n",
      "finished 16\n",
      "finished 17\n",
      "finished 18\n",
      "finished 19\n",
      "finished 20\n",
      "finished 21\n",
      "finished 22\n",
      "finished 23\n",
      "finished 24\n",
      "finished 25\n",
      "finished 26\n",
      "finished 27\n",
      "finished 28\n",
      "finished 29\n",
      "finished 30\n",
      "finished 31\n",
      "finished 32\n",
      "finished 33\n",
      "finished 34\n",
      "finished 35\n",
      "finished 36\n",
      "finished 37\n",
      "finished 38\n",
      "finished 39\n",
      "finished 40\n",
      "finished 41\n",
      "finished 42\n",
      "finished 43\n",
      "finished 44\n",
      "finished 45\n",
      "finished 46\n",
      "finished 47\n",
      "finished 48\n",
      "finished 49\n",
      "finished 50\n",
      "finished 51\n",
      "finished 52\n",
      "finished 53\n",
      "finished 54\n",
      "finished 55\n",
      "finished 56\n",
      "finished 57\n",
      "finished 58\n",
      "finished 59\n",
      "finished 60\n",
      "finished 61\n",
      "finished 62\n",
      "finished 63\n",
      "finished 64\n",
      "finished 65\n",
      "finished 66\n",
      "finished 67\n",
      "finished 68\n",
      "finished 69\n",
      "finished 70\n",
      "finished 71\n",
      "finished 72\n",
      "finished 73\n",
      "finished 74\n",
      "finished 75\n",
      "finished 76\n",
      "finished 77\n",
      "finished 78\n",
      "finished 79\n",
      "finished 80\n",
      "finished 81\n",
      "finished 82\n",
      "finished 83\n",
      "finished 84\n",
      "finished 85\n",
      "finished 86\n",
      "finished 87\n",
      "finished 88\n",
      "finished 89\n",
      "finished 90\n",
      "finished 91\n",
      "finished 92\n",
      "finished 93\n",
      "finished 94\n",
      "finished 95\n",
      "finished 96\n",
      "finished 97\n",
      "finished 98\n",
      "finished 99\n",
      "finished 100\n",
      "finished 101\n",
      "finished 102\n",
      "finished 103\n",
      "finished 104\n",
      "finished 105\n",
      "finished 106\n",
      "finished 107\n",
      "finished 108\n",
      "finished 109\n",
      "finished 110\n",
      "finished 111\n",
      "finished 112\n",
      "finished 113\n",
      "finished 114\n",
      "finished 115\n",
      "finished 116\n",
      "finished 117\n",
      "finished 118\n",
      "finished 119\n",
      "finished 120\n",
      "finished 121\n",
      "finished 122\n",
      "finished 123\n",
      "finished 124\n",
      "finished 125\n",
      "finished 126\n",
      "finished 127\n",
      "finished 128\n",
      "finished 129\n",
      "finished 130\n",
      "finished 131\n",
      "finished 132\n",
      "finished 133\n",
      "finished 134\n",
      "finished 135\n",
      "finished 136\n",
      "finished 137\n",
      "finished 138\n",
      "finished 139\n",
      "finished 140\n",
      "finished 141\n",
      "finished 142\n",
      "finished 143\n",
      "finished 144\n",
      "finished 145\n",
      "finished 146\n",
      "finished 147\n",
      "finished 148\n",
      "finished 149\n",
      "finished 150\n",
      "finished 151\n",
      "finished 152\n",
      "finished 153\n",
      "finished 154\n",
      "finished 155\n",
      "finished 156\n",
      "finished 157\n",
      "finished 158\n",
      "finished 159\n",
      "finished 160\n",
      "finished 161\n",
      "finished 162\n",
      "finished 163\n",
      "finished 164\n",
      "finished 165\n",
      "finished 166\n",
      "finished 167\n",
      "finished 168\n",
      "finished 169\n",
      "finished 170\n",
      "finished 171\n",
      "finished 172\n",
      "finished 173\n",
      "finished 174\n",
      "finished 175\n",
      "finished 176\n",
      "finished 177\n",
      "finished 178\n",
      "finished 179\n",
      "finished 180\n",
      "finished 181\n",
      "finished 182\n",
      "finished 183\n",
      "finished 184\n",
      "finished 185\n",
      "finished 186\n",
      "finished 187\n",
      "finished 188\n",
      "finished 189\n",
      "finished 190\n",
      "finished 191\n",
      "finished 192\n",
      "finished 193\n",
      "finished 194\n",
      "finished 195\n",
      "finished 196\n",
      "finished 197\n",
      "finished 198\n",
      "finished 199\n",
      "finished 200\n",
      "finished 201\n",
      "finished 202\n",
      "finished 203\n",
      "finished 204\n",
      "finished 205\n",
      "finished 206\n",
      "finished 207\n",
      "finished 208\n",
      "finished 209\n",
      "finished 210\n",
      "finished 211\n",
      "finished 212\n",
      "finished 213\n",
      "finished 214\n",
      "finished 215\n",
      "finished 216\n",
      "finished 217\n",
      "finished 218\n",
      "finished 219\n",
      "finished 220\n",
      "finished 221\n",
      "finished 222\n",
      "finished 223\n",
      "finished 224\n",
      "finished 225\n",
      "finished 226\n",
      "finished 227\n",
      "finished 228\n",
      "finished 229\n",
      "finished 230\n",
      "finished 231\n",
      "finished 232\n",
      "finished 233\n",
      "finished 234\n",
      "finished 235\n",
      "finished 236\n",
      "finished 237\n",
      "finished 238\n",
      "finished 239\n",
      "finished 240\n",
      "finished 241\n",
      "finished 242\n",
      "finished 243\n",
      "finished 244\n",
      "finished 245\n",
      "finished 246\n",
      "finished 247\n",
      "finished 248\n",
      "finished 249\n",
      "finished 250\n",
      "finished 251\n",
      "finished 252\n",
      "finished 253\n",
      "finished 254\n",
      "finished 255\n",
      "finished 256\n",
      "finished 257\n",
      "finished 258\n",
      "finished 259\n",
      "finished 260\n",
      "finished 261\n",
      "finished 262\n",
      "finished 263\n",
      "finished 264\n",
      "finished 265\n",
      "finished 266\n",
      "finished 267\n",
      "finished 268\n",
      "finished 269\n",
      "finished 270\n",
      "finished 271\n",
      "finished 272\n",
      "finished 273\n",
      "finished 274\n",
      "finished 275\n",
      "finished 276\n",
      "finished 277\n",
      "finished 278\n",
      "finished 279\n",
      "finished 280\n",
      "finished 281\n",
      "finished 282\n",
      "finished 283\n",
      "finished 284\n",
      "finished 285\n",
      "finished 286\n",
      "finished 287\n",
      "finished 288\n",
      "finished 289\n",
      "finished 290\n",
      "finished 291\n",
      "finished 292\n",
      "finished 293\n",
      "finished 294\n",
      "finished 295\n",
      "finished 296\n",
      "finished 297\n",
      "finished 298\n",
      "finished 299\n",
      "finished 300\n",
      "finished 301\n",
      "finished 302\n",
      "finished 303\n",
      "finished 304\n",
      "finished 305\n",
      "finished 306\n",
      "finished 307\n",
      "finished 308\n",
      "finished 309\n",
      "finished 310\n",
      "finished 311\n",
      "finished 312\n",
      "finished 313\n",
      "finished 314\n",
      "finished 315\n",
      "finished 316\n",
      "finished 317\n",
      "finished 318\n",
      "finished 319\n",
      "finished 320\n",
      "finished 321\n",
      "finished 322\n",
      "finished 323\n",
      "finished 324\n",
      "finished 325\n",
      "finished 326\n",
      "finished 327\n",
      "finished 328\n",
      "finished 329\n",
      "finished 330\n",
      "finished 331\n",
      "finished 332\n",
      "finished 333\n",
      "finished 334\n",
      "finished 335\n",
      "finished 336\n",
      "finished 337\n",
      "finished 338\n",
      "finished 339\n",
      "finished 340\n",
      "finished 341\n",
      "finished 342\n",
      "finished 343\n",
      "finished 344\n",
      "finished 345\n",
      "finished 346\n",
      "finished 347\n",
      "finished 348\n",
      "finished 349\n",
      "finished 350\n",
      "finished 351\n",
      "finished 352\n",
      "finished 353\n",
      "finished 354\n",
      "finished 355\n",
      "finished 356\n",
      "finished 357\n",
      "finished 358\n",
      "finished 359\n",
      "finished 360\n",
      "finished 361\n",
      "finished 362\n",
      "finished 363\n",
      "finished 364\n",
      "finished 365\n",
      "finished 366\n",
      "finished 367\n",
      "finished 368\n",
      "finished 369\n",
      "finished 370\n",
      "finished 371\n",
      "finished 372\n",
      "finished 373\n",
      "finished 374\n",
      "finished 375\n",
      "finished 376\n",
      "finished 377\n",
      "finished 378\n",
      "finished 379\n",
      "finished 380\n",
      "finished 381\n",
      "finished 382\n",
      "finished 383\n",
      "finished 384\n",
      "finished 385\n",
      "finished 386\n",
      "finished 387\n",
      "finished 388\n",
      "finished 389\n",
      "finished 390\n",
      "finished 391\n",
      "finished 392\n",
      "finished 393\n",
      "finished 394\n",
      "finished 395\n",
      "finished 396\n",
      "finished 397\n",
      "finished 398\n",
      "finished 399\n",
      "finished 400\n",
      "finished 401\n",
      "finished 402\n",
      "finished 403\n",
      "finished 404\n",
      "finished 405\n",
      "finished 406\n",
      "finished 407\n",
      "finished 408\n",
      "finished 409\n",
      "finished 410\n",
      "finished 411\n",
      "finished 412\n",
      "finished 413\n",
      "finished 414\n",
      "finished 415\n",
      "finished 416\n",
      "finished 417\n",
      "finished 418\n",
      "finished 419\n",
      "finished 420\n",
      "finished 421\n",
      "finished 422\n",
      "finished 423\n",
      "finished 424\n",
      "finished 425\n",
      "finished 426\n",
      "finished 427\n",
      "finished 428\n",
      "finished 429\n",
      "finished 430\n",
      "finished 431\n",
      "finished 432\n",
      "finished 433\n",
      "finished 434\n",
      "finished 435\n",
      "finished 436\n",
      "finished 437\n",
      "finished 438\n",
      "finished 439\n",
      "finished 440\n",
      "finished 441\n",
      "finished 442\n",
      "finished 443\n",
      "finished 444\n",
      "finished 445\n",
      "finished 446\n",
      "finished 447\n",
      "finished 448\n",
      "finished 449\n",
      "finished 450\n",
      "finished 451\n",
      "finished 452\n",
      "finished 453\n",
      "finished 454\n",
      "finished 455\n",
      "finished 456\n",
      "finished 457\n",
      "finished 458\n",
      "finished 459\n",
      "finished 460\n",
      "finished 461\n",
      "finished 462\n",
      "finished 463\n",
      "finished 464\n",
      "finished 465\n",
      "finished 466\n",
      "finished 467\n",
      "finished 468\n",
      "finished 469\n",
      "finished 470\n",
      "finished 471\n",
      "finished 472\n",
      "finished 473\n",
      "finished 474\n",
      "finished 475\n",
      "finished 476\n",
      "finished 477\n",
      "finished 478\n",
      "finished 479\n",
      "finished 480\n",
      "finished 481\n",
      "finished 482\n",
      "finished 483\n",
      "finished 484\n",
      "finished 485\n",
      "finished 486\n",
      "finished 487\n",
      "finished 488\n",
      "finished 489\n",
      "finished 490\n",
      "finished 491\n",
      "finished 492\n",
      "finished 493\n",
      "finished 494\n",
      "finished 495\n",
      "finished 496\n",
      "finished 497\n",
      "finished 498\n",
      "finished 499\n",
      "finished 500\n",
      "finished 501\n",
      "finished 502\n",
      "finished 503\n",
      "finished 504\n",
      "finished 505\n",
      "finished 506\n",
      "finished 507\n",
      "finished 508\n",
      "finished 509\n",
      "finished 510\n",
      "finished 511\n",
      "finished 512\n",
      "finished 513\n",
      "finished 514\n",
      "finished 515\n",
      "finished 516\n",
      "finished 517\n",
      "finished 518\n",
      "finished 519\n",
      "finished 520\n",
      "finished 521\n",
      "finished 522\n",
      "finished 523\n",
      "finished 524\n",
      "finished 525\n",
      "finished 526\n",
      "finished 527\n",
      "finished 528\n",
      "finished 529\n",
      "finished 530\n",
      "finished 531\n",
      "finished 532\n",
      "finished 533\n",
      "finished 534\n",
      "finished 535\n",
      "finished 536\n",
      "finished 537\n",
      "finished 538\n",
      "finished 539\n",
      "finished 540\n",
      "finished 541\n",
      "finished 542\n",
      "finished 543\n",
      "finished 544\n",
      "finished 545\n",
      "finished 546\n",
      "finished 547\n",
      "finished 548\n",
      "finished 549\n",
      "finished 550\n",
      "finished 551\n",
      "finished 552\n",
      "finished 553\n",
      "finished 554\n",
      "finished 555\n",
      "finished 556\n",
      "finished 557\n",
      "finished 558\n",
      "finished 559\n",
      "finished 560\n",
      "finished 561\n",
      "finished 562\n",
      "finished 563\n",
      "finished 564\n",
      "finished 565\n",
      "finished 566\n",
      "finished 567\n",
      "finished 568\n",
      "finished 569\n",
      "finished 570\n",
      "finished 571\n",
      "finished 572\n",
      "finished 573\n",
      "finished 574\n",
      "finished 575\n",
      "finished 576\n",
      "finished 577\n",
      "finished 578\n",
      "finished 579\n",
      "finished 580\n",
      "finished 581\n",
      "finished 582\n",
      "finished 583\n",
      "finished 584\n",
      "finished 585\n",
      "finished 586\n",
      "finished 587\n",
      "finished 588\n",
      "finished 589\n",
      "finished 590\n",
      "finished 591\n",
      "finished 592\n",
      "finished 593\n",
      "finished 594\n",
      "finished 595\n",
      "finished 596\n",
      "finished 597\n",
      "finished 598\n",
      "finished 599\n",
      "finished 600\n",
      "finished 601\n",
      "finished 602\n",
      "finished 603\n",
      "finished 604\n",
      "finished 605\n",
      "finished 606\n",
      "finished 607\n",
      "finished 608\n",
      "finished 609\n",
      "finished 610\n",
      "finished 611\n",
      "finished 612\n",
      "finished 613\n",
      "finished 614\n",
      "finished 615\n",
      "finished 616\n",
      "finished 617\n",
      "finished 618\n",
      "finished 619\n",
      "finished 620\n",
      "finished 621\n",
      "finished 622\n",
      "finished 623\n",
      "finished 624\n",
      "finished 625\n",
      "finished 626\n",
      "finished 627\n",
      "finished 628\n",
      "finished 629\n",
      "finished 630\n",
      "finished 631\n",
      "finished 632\n",
      "finished 633\n",
      "finished 634\n",
      "finished 635\n",
      "finished 636\n",
      "finished 637\n",
      "finished 638\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished 639\n",
      "finished 640\n",
      "finished 641\n",
      "finished 642\n",
      "finished 643\n",
      "finished 644\n",
      "finished 645\n",
      "finished 646\n",
      "finished 647\n",
      "finished 648\n",
      "finished 649\n",
      "finished 650\n",
      "finished 651\n",
      "finished 652\n",
      "finished 653\n",
      "finished 654\n",
      "finished 655\n",
      "finished 656\n",
      "finished 657\n",
      "finished 658\n",
      "finished 659\n",
      "finished 660\n",
      "finished 661\n",
      "finished 662\n",
      "finished 663\n",
      "finished 664\n",
      "finished 665\n",
      "finished 666\n",
      "finished 667\n",
      "finished 668\n",
      "finished 669\n",
      "finished 670\n",
      "finished 671\n",
      "finished 672\n",
      "finished 673\n",
      "finished 674\n",
      "finished 675\n",
      "finished 676\n",
      "finished 677\n",
      "finished 678\n",
      "finished 679\n",
      "finished 680\n",
      "finished 681\n",
      "finished 682\n",
      "finished 683\n",
      "finished 684\n",
      "finished 685\n",
      "finished 686\n",
      "finished 687\n",
      "finished 688\n",
      "finished 689\n",
      "finished 690\n",
      "finished 691\n",
      "finished 692\n",
      "finished 693\n",
      "finished 694\n",
      "finished 695\n",
      "finished 696\n",
      "finished 697\n",
      "finished 698\n",
      "finished 699\n",
      "finished 700\n",
      "finished 701\n",
      "finished 702\n",
      "finished 703\n",
      "finished 704\n",
      "finished 705\n",
      "finished 706\n",
      "finished 707\n",
      "finished 708\n",
      "finished 709\n",
      "finished 710\n",
      "finished 711\n",
      "finished 712\n",
      "finished 713\n",
      "finished 714\n",
      "finished 715\n",
      "finished 716\n",
      "finished 717\n",
      "finished 718\n",
      "finished 719\n",
      "finished 720\n",
      "finished 721\n",
      "finished 722\n",
      "finished 723\n",
      "finished 724\n",
      "finished 725\n",
      "finished 726\n",
      "finished 727\n",
      "finished 728\n",
      "finished 729\n",
      "finished 730\n",
      "finished 731\n",
      "finished 732\n",
      "finished 733\n",
      "finished 734\n",
      "finished 735\n",
      "finished 736\n",
      "finished 737\n",
      "finished 738\n",
      "finished 739\n",
      "finished 740\n",
      "finished 741\n",
      "finished 742\n",
      "finished 743\n",
      "finished 744\n",
      "finished 745\n",
      "finished 746\n",
      "finished 747\n",
      "finished 748\n",
      "finished 749\n",
      "finished 750\n",
      "finished 751\n",
      "finished 752\n",
      "finished 753\n",
      "finished 754\n",
      "finished 755\n",
      "finished 756\n",
      "finished 757\n",
      "finished 758\n",
      "finished 759\n",
      "finished 760\n",
      "finished 761\n",
      "finished 762\n",
      "finished 763\n",
      "finished 764\n",
      "finished 765\n",
      "finished 766\n",
      "finished 767\n",
      "finished 768\n",
      "finished 769\n",
      "finished 770\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12436\\1462970014.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0msent_num\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_split\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdoc_num\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mcur_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnagisa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtagging\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_split\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdoc_num\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msent_num\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcur_text\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[0mpostags\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcur_text\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpostags\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mchar_offset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\nagisa\\tagger.py\u001b[0m in \u001b[0;36mwords\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    248\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    249\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__words\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 250\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__wakati\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__lower\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    251\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__words\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\nagisa\\tagger.py\u001b[0m in \u001b[0;36mwakati\u001b[1;34m(self, text, lower)\u001b[0m\n\u001b[0;32m     75\u001b[0m                                          window_size=self._hp['WINDOW_SIZE'])\n\u001b[0;32m     76\u001b[0m         \u001b[0mobs\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode_ws\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeats\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m         \u001b[0mobs\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnpvalue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mob\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mobs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     78\u001b[0m         \u001b[0mtags\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnp_viterbi\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrans_array\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\nagisa\\tagger.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     75\u001b[0m                                          window_size=self._hp['WINDOW_SIZE'])\n\u001b[0;32m     76\u001b[0m         \u001b[0mobs\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode_ws\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeats\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m         \u001b[0mobs\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnpvalue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mob\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mobs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     78\u001b[0m         \u001b[0mtags\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnp_viterbi\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrans_array\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "labeled_data = []\n",
    "# first 1000 due to data/time limitations\n",
    "for doc_num in range(1000):\n",
    "    for sent_num in range(len(data_split[doc_num])):\n",
    "        cur_text = nagisa.tagging(data_split[doc_num][sent_num])\n",
    "        words = cur_text.words\n",
    "        postags = cur_text.postags\n",
    "        char_offset = 0\n",
    "        for i in range(len(words)):\n",
    "            labeled_data.append([doc_num, sent_num, char_offset, words[i], postags[i]])\n",
    "            char_offset += 1\n",
    "    print(\"finished \" + str(doc_num))\n",
    "labeled_df = pd.DataFrame(labeled_data)\n",
    "labeled_df.to_pickle('labeledDataNew.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dae6acc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_df = pd.DataFrame(labeled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "721826b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_df.columns = ['docNum','sentNum','charOffset','word','posTag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "afc2babd",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_df.to_pickle('labeledDataNew.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18440ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_df = pd.read_pickle('labeledDataNew.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "617a1d69",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docNum</th>\n",
       "      <th>sentNum</th>\n",
       "      <th>charOffset</th>\n",
       "      <th>word</th>\n",
       "      <th>posTag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>よう</td>\n",
       "      <td>形容詞</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>こそ</td>\n",
       "      <td>助詞</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>ウィキペディア</td>\n",
       "      <td>名詞</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>空白</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-</td>\n",
       "      <td>補助記号</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9909358</th>\n",
       "      <td>771</td>\n",
       "      <td>1300</td>\n",
       "      <td>18</td>\n",
       "      <td>を</td>\n",
       "      <td>助詞</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9909359</th>\n",
       "      <td>771</td>\n",
       "      <td>1300</td>\n",
       "      <td>19</td>\n",
       "      <td>受け</td>\n",
       "      <td>動詞</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9909360</th>\n",
       "      <td>771</td>\n",
       "      <td>1300</td>\n",
       "      <td>20</td>\n",
       "      <td>て</td>\n",
       "      <td>助詞</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9909361</th>\n",
       "      <td>771</td>\n",
       "      <td>1300</td>\n",
       "      <td>21</td>\n",
       "      <td>回復</td>\n",
       "      <td>名詞</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9909362</th>\n",
       "      <td>771</td>\n",
       "      <td>1300</td>\n",
       "      <td>22</td>\n",
       "      <td>する</td>\n",
       "      <td>動詞</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9909363 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         docNum  sentNum  charOffset     word posTag\n",
       "0             0        0           0       よう    形容詞\n",
       "1             0        0           1       こそ     助詞\n",
       "2             0        1           0  ウィキペディア     名詞\n",
       "3             0        1           1        　     空白\n",
       "4             0        1           2        -   補助記号\n",
       "...         ...      ...         ...      ...    ...\n",
       "9909358     771     1300          18        を     助詞\n",
       "9909359     771     1300          19       受け     動詞\n",
       "9909360     771     1300          20        て     助詞\n",
       "9909361     771     1300          21       回復     名詞\n",
       "9909362     771     1300          22       する     動詞\n",
       "\n",
       "[9909363 rows x 5 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58dcd7d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "45ecb078",
   "metadata": {},
   "source": [
    "## doc2vec\n",
    "\n",
    "Roughly following the outline of https://towardsdatascience.com/multi-class-text-classification-with-doc2vec-logistic-regression-9da9947b43f4#:~:text=Doc2vec%20is%20an%20NLP%20tool,of%20scope%20of%20this%20article\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6867a81",
   "metadata": {},
   "source": [
    "### Word Tokenizer on space delimited documents (without sentence context)\n",
    "\n",
    "The first application of doc2vec will apply the word tokenizer to a dataset of space delimited documents, with sentence delimiters removed and ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1989956a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create space delimited dataset from the nagisa labeled dataframe\n",
    "space_delimited = []\n",
    "for i in range(772):\n",
    "    concat_doc = ' '.join(list(labeled_df[(labeled_df['docNum']==i)]['word']))\n",
    "    space_delimited.append(concat_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c02977b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>よう こそ ウィキペディア 　 - 　 ウィキペディア 日本 語 版 　 - 　 百 科 事...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>星野 　 源 ( ほしの 　 みな もと 、 1 9 8 1 年 1 月 2 8 日 　 -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>真田 　 信繁 ( さなだ 　 のぶしげ ) は 、 安土 桃山 時代 から 江戸 時代 初...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>高橋 　 一生 ( たか は し 　 いっせい 、 英字 表記 : Issey 　 Taka...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>『 君 の 名 は 』 ( きみ の な は 、 英 : 　 Your 　 Name ) は...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>767</th>\n",
       "      <td>VX ガス ( ヴィーエックス ・ ガス 、 V X 、 O - エチル - S - ( 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>768</th>\n",
       "      <td>上杉 　 景勝 ( うえ すぎ 　 かげかつ ) は 、 戦国 時代 から 江戸 時代 前期...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>769</th>\n",
       "      <td>有賀 　 さつき ( ありが 　 さつき 、 1 9 6 5 年 9 月 9 日 　 - 　...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>770</th>\n",
       "      <td>髙安 　 晃 ( たかやす 　 あきら 、 1 9 9 0 年 〈 平成 2 年 〉 2 月...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>771</th>\n",
       "      <td>護廷 十 三 隊 ( ごていじゅう さん たい ) は 、 漫画 『 BLEACH 』 に ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>772 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     0\n",
       "0    よう こそ ウィキペディア 　 - 　 ウィキペディア 日本 語 版 　 - 　 百 科 事...\n",
       "1    星野 　 源 ( ほしの 　 みな もと 、 1 9 8 1 年 1 月 2 8 日 　 -...\n",
       "2    真田 　 信繁 ( さなだ 　 のぶしげ ) は 、 安土 桃山 時代 から 江戸 時代 初...\n",
       "3    高橋 　 一生 ( たか は し 　 いっせい 、 英字 表記 : Issey 　 Taka...\n",
       "4    『 君 の 名 は 』 ( きみ の な は 、 英 : 　 Your 　 Name ) は...\n",
       "..                                                 ...\n",
       "767  VX ガス ( ヴィーエックス ・ ガス 、 V X 、 O - エチル - S - ( 2...\n",
       "768  上杉 　 景勝 ( うえ すぎ 　 かげかつ ) は 、 戦国 時代 から 江戸 時代 前期...\n",
       "769  有賀 　 さつき ( ありが 　 さつき 、 1 9 6 5 年 9 月 9 日 　 - 　...\n",
       "770  髙安 　 晃 ( たかやす 　 あきら 、 1 9 9 0 年 〈 平成 2 年 〉 2 月...\n",
       "771  護廷 十 三 隊 ( ごていじゅう さん たい ) は 、 漫画 『 BLEACH 』 に ...\n",
       "\n",
       "[772 rows x 1 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new dataframe from the space delimited text\n",
    "dfdf = pd.DataFrame(space_delimited)\n",
    "dfdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "575aad9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = ['あそこ','あっ','あの','あのかた','あの人','あり','あります','ある','あれ','い','いう','います','いる','う','うち','え','お','および','おり','おります','か','かつて','から','が','き','ここ','こちら','こと','この','これ','これら','さ','さらに','し','しかし','する','ず','せ','せる','そこ','そして','その','その他','その後','それ','それぞれ','それで','た','ただし','たち','ため','たり','だ','だっ','だれ','つ','て','で','でき','できる','です','では','でも','と','という','といった','とき','ところ','として','とともに','とも','と共に','どこ','どの','な','ない','なお','なかっ','ながら','なく','なっ','など','なに','なら','なり','なる','なん','に','において','における','について','にて','によって','により','による','に対して','に対する','に関する','の','ので','のみ','は','ば','へ','ほか','ほとんど','ほど','ます','また','または','まで','も','もの','ものの','や','よう','より','ら','られ','られる','れ','れる','を','ん','何','及び','彼','彼女','我々','特に','私','私達','貴方','貴方方''ようこそ','ウィキペディア','ウィキペディア日本語版','百科事典目次','検索資料','空白','補助記号','）', '（', '、', '『', '』', '・','：', '／', '＋', '→', '「', '」', ' ', '-', '/', '\\\\', '。', '\\n', '！']\n",
    "stop += list(string.punctuation)\n",
    "\n",
    "# Tokenize the text using nltk; tag documents with integers, as we do not have labeled data\n",
    "tagged = [TaggedDocument(nltk.word_tokenize(doc), [i]) for i, doc in enumerate(list(dfdf[0]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bde5756c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set notebook to use available cores\n",
    "cores = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e634c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Doc2Vec model\n",
    "model_dbow = Doc2Vec(dm=0, vector_size=300, negative=5, hs=0, min_count=200, sample = 0, workers=cores)\n",
    "model_dbow.build_vocab([x for x in tagged])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb13fe3e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Train Distributed Bag of Words (DBOW) model\n",
    "for epoch in range(30):\n",
    "    model_dbow.train(utils.shuffle([x for x in tagged]), total_examples=len(tagged), epochs=1)\n",
    "    model_dbow.alpha -= 0.002\n",
    "    model_dbow.min_alpha = model_dbow.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "38f32fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vector for each document\n",
    "def vec_for_learning(model, tagged_docs):\n",
    "    sents = tagged_docs\n",
    "    targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words)) for doc in sents])\n",
    "    return targets, regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "70bf68a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute feature vectors for all documents\n",
    "t, x = vec_for_learning(model_dbow, tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "bff8df58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform k-means clustering over feature vectors\n",
    "kmeans = KMeans(init=\"k-means++\", n_clusters=15)\n",
    "kmeans.fit(x)\n",
    "labels = kmeans.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e80026df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>labels</th>\n",
       "      <th>article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>よう こそ ウィキペディア 　 - 　 ウィキペディア 日本 語 版 　 - 　 百 科 事...</td>\n",
       "      <td>1</td>\n",
       "      <td>メインページ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>星野 　 源 ( ほしの 　 みな もと 、 1 9 8 1 年 1 月 2 8 日 　 -...</td>\n",
       "      <td>5</td>\n",
       "      <td>星野源</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>真田 　 信繁 ( さなだ 　 のぶしげ ) は 、 安土 桃山 時代 から 江戸 時代 初...</td>\n",
       "      <td>2</td>\n",
       "      <td>真田信繁</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>高橋 　 一生 ( たか は し 　 いっせい 、 英字 表記 : Issey 　 Taka...</td>\n",
       "      <td>13</td>\n",
       "      <td>高橋一生</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>『 君 の 名 は 』 ( きみ の な は 、 英 : 　 Your 　 Name ) は...</td>\n",
       "      <td>9</td>\n",
       "      <td>君の名は。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>767</th>\n",
       "      <td>VX ガス ( ヴィーエックス ・ ガス 、 V X 、 O - エチル - S - ( 2...</td>\n",
       "      <td>12</td>\n",
       "      <td>VXガス</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>768</th>\n",
       "      <td>上杉 　 景勝 ( うえ すぎ 　 かげかつ ) は 、 戦国 時代 から 江戸 時代 前期...</td>\n",
       "      <td>12</td>\n",
       "      <td>上杉景勝</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>769</th>\n",
       "      <td>有賀 　 さつき ( ありが 　 さつき 、 1 9 6 5 年 9 月 9 日 　 - 　...</td>\n",
       "      <td>10</td>\n",
       "      <td>有賀さつき</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>770</th>\n",
       "      <td>髙安 　 晃 ( たかやす 　 あきら 、 1 9 9 0 年 〈 平成 2 年 〉 2 月...</td>\n",
       "      <td>12</td>\n",
       "      <td>高安晃</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>771</th>\n",
       "      <td>護廷 十 三 隊 ( ごていじゅう さん たい ) は 、 漫画 『 BLEACH 』 に ...</td>\n",
       "      <td>14</td>\n",
       "      <td>護廷十三隊</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>772 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     0  labels article\n",
       "0    よう こそ ウィキペディア 　 - 　 ウィキペディア 日本 語 版 　 - 　 百 科 事...       1  メインページ\n",
       "1    星野 　 源 ( ほしの 　 みな もと 、 1 9 8 1 年 1 月 2 8 日 　 -...       5     星野源\n",
       "2    真田 　 信繁 ( さなだ 　 のぶしげ ) は 、 安土 桃山 時代 から 江戸 時代 初...       2    真田信繁\n",
       "3    高橋 　 一生 ( たか は し 　 いっせい 、 英字 表記 : Issey 　 Taka...      13    高橋一生\n",
       "4    『 君 の 名 は 』 ( きみ の な は 、 英 : 　 Your 　 Name ) は...       9   君の名は。\n",
       "..                                                 ...     ...     ...\n",
       "767  VX ガス ( ヴィーエックス ・ ガス 、 V X 、 O - エチル - S - ( 2...      12    VXガス\n",
       "768  上杉 　 景勝 ( うえ すぎ 　 かげかつ ) は 、 戦国 時代 から 江戸 時代 前期...      12    上杉景勝\n",
       "769  有賀 　 さつき ( ありが 　 さつき 、 1 9 6 5 年 9 月 9 日 　 - 　...      10   有賀さつき\n",
       "770  髙安 　 晃 ( たかやす 　 あきら 、 1 9 9 0 年 〈 平成 2 年 〉 2 月...      12     高安晃\n",
       "771  護廷 十 三 隊 ( ごていじゅう さん たい ) は 、 漫画 『 BLEACH 』 に ...      14   護廷十三隊\n",
       "\n",
       "[772 rows x 3 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add labels and article titles to dataframe\n",
    "dfdf['labels'] = labels\n",
    "dfdf['article'] = list(wiki_df['article'].iloc[0:772])\n",
    "dfdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "060c5e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group article text by cluster\n",
    "clustered_articles = [list(dfdf[dfdf['labels']==i][0]) for i in range(15)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6b6e1ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the most frequently used words in each cluster of documents\n",
    "\n",
    "stop = ['あそこ','あっ','あの','あのかた','あの人','あり','あります','ある','あれ','い','いう','います','いる','う','うち','え','お','および','おり','おります','か','かつて','から','が','き','ここ','こちら','こと','この','これ','これら','さ','さらに','し','しかし','する','ず','せ','せる','そこ','そして','その','その他','その後','それ','それぞれ','それで','た','ただし','たち','ため','たり','だ','だっ','だれ','つ','て','で','でき','できる','です','では','でも','と','という','といった','とき','ところ','として','とともに','とも','と共に','どこ','どの','な','ない','なお','なかっ','ながら','なく','なっ','など','なに','なら','なり','なる','なん','に','において','における','について','にて','によって','により','による','に対して','に対する','に関する','の','ので','のみ','は','ば','へ','ほか','ほとんど','ほど','ます','また','または','まで','も','もの','ものの','や','よう','より','ら','られ','られる','れ','れる','を','ん','何','及び','彼','彼女','我々','特に','私','私達','貴方','貴方方''ようこそ','ウィキペディア','ウィキペディア日本語版','百科事典目次','検索資料','空白','補助記号','）', '（', '、', '『', '』', '・','：', '／', '＋', '→', '「', '」', ' ', '-', '/', '\\\\', '。', '\\n', '！', '年', '月', '日', '\\u3000', '第', '〜', '者', '人', '的', '〈', '〉', '一', '中']\n",
    "stop += list(string.punctuation)\n",
    "stop += list(string.digits)\n",
    "# had to add lots of extra stop words to eliminate frequently-used, but not meaningful terms\n",
    "\n",
    "topic_words = []\n",
    "for topic in range(15):\n",
    "    common_chars = collections.Counter()\n",
    "    for article_text in clustered_articles[topic]:\n",
    "        article_list = article_text.split(' ')\n",
    "        for word in article_list:\n",
    "            if word not in stop:\n",
    "                common_chars[word] += 1\n",
    "    topic_words.append(common_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "02fa9782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common words in topic 0\n",
      "['役', '日本', '話', 'テレビ', '後', '編', '声', '版', '党', '登場']\n",
      "\n",
      "Most common words in topic 1\n",
      "['後', '巨人', '信長', '家', '役', '日本', 'テレビ', '安倍', '話', '国']\n",
      "\n",
      "Most common words in topic 2\n",
      "['後', 'テレビ', '役', '話', '日本', '回', '放送', '時', '登場', '声']\n",
      "\n",
      "Most common words in topic 3\n",
      "['役', '後', '戦車', '日本', 'テレビ', '版', '声', '回', 'DVD', 'シリーズ']\n",
      "\n",
      "Most common words in topic 4\n",
      "['市', '都市', '指定', '回', '加野', '後', '戦', '万', 'あさ', '屋']\n",
      "\n",
      "Most common words in topic 5\n",
      "['役', 'テレビ', '後', '日本', '会', '話', '組', '達', '目', '回']\n",
      "\n",
      "Most common words in topic 6\n",
      "['中国', '人民', '国', '中華', '共和', '銀行', '政府', '化', '世界', '部']\n",
      "\n",
      "Most common words in topic 7\n",
      "['後', '桐生', 'ライダー', '仮面', '話', 'S', '役', '登場', 'テレビ', '会']\n",
      "\n",
      "Most common words in topic 8\n",
      "['テレビ', '日本', 'シリーズ', '戦', '後', '役', '三', '回', '登場', 'ルパン']\n",
      "\n",
      "Most common words in topic 9\n",
      "['日本', 'テレビ', '後', '役', '放送', '回', '三島', '生', '話', 'ベトナム']\n",
      "\n",
      "Most common words in topic 10\n",
      "['役', '後', 'テレビ', '話', '日本', '声', '時', '版', '目', '”']\n",
      "\n",
      "Most common words in topic 11\n",
      "['後', 'テレビ', '版', '日本', '役', '声', '話', '時', 'アニメ', '世界']\n",
      "\n",
      "Most common words in topic 12\n",
      "['後', 'テレビ', '回', '放送', '日本', '話', '役', '版', '時', '魔法']\n",
      "\n",
      "Most common words in topic 13\n",
      "['日本', '役', 'テレビ', '後', '東京', '回', '韓国', '監督', '軍', '放送']\n",
      "\n",
      "Most common words in topic 14\n",
      "['テレビ', '日本', '版', '後', '役', '期', '回', '※', '話', '時']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print most common words in each topic cluster of documents\n",
    "for topic in range(15):\n",
    "    print('Most common words in topic ' + str(topic))\n",
    "    print([topic_words[topic].most_common(10)[i][0] for i in range(10)])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e730dd72",
   "metadata": {},
   "source": [
    "Use 5 clusters instead of 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "febc0640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform k-means clustering over feature vectors\n",
    "kmeans = KMeans(init=\"k-means++\", n_clusters=5)\n",
    "kmeans.fit(x)\n",
    "labels = kmeans.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "76338d4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>labels</th>\n",
       "      <th>article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>よう こそ ウィキペディア 　 - 　 ウィキペディア 日本 語 版 　 - 　 百 科 事...</td>\n",
       "      <td>1</td>\n",
       "      <td>メインページ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>星野 　 源 ( ほしの 　 みな もと 、 1 9 8 1 年 1 月 2 8 日 　 -...</td>\n",
       "      <td>1</td>\n",
       "      <td>星野源</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>真田 　 信繁 ( さなだ 　 のぶしげ ) は 、 安土 桃山 時代 から 江戸 時代 初...</td>\n",
       "      <td>2</td>\n",
       "      <td>真田信繁</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>高橋 　 一生 ( たか は し 　 いっせい 、 英字 表記 : Issey 　 Taka...</td>\n",
       "      <td>2</td>\n",
       "      <td>高橋一生</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>『 君 の 名 は 』 ( きみ の な は 、 英 : 　 Your 　 Name ) は...</td>\n",
       "      <td>1</td>\n",
       "      <td>君の名は。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>767</th>\n",
       "      <td>VX ガス ( ヴィーエックス ・ ガス 、 V X 、 O - エチル - S - ( 2...</td>\n",
       "      <td>1</td>\n",
       "      <td>VXガス</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>768</th>\n",
       "      <td>上杉 　 景勝 ( うえ すぎ 　 かげかつ ) は 、 戦国 時代 から 江戸 時代 前期...</td>\n",
       "      <td>4</td>\n",
       "      <td>上杉景勝</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>769</th>\n",
       "      <td>有賀 　 さつき ( ありが 　 さつき 、 1 9 6 5 年 9 月 9 日 　 - 　...</td>\n",
       "      <td>4</td>\n",
       "      <td>有賀さつき</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>770</th>\n",
       "      <td>髙安 　 晃 ( たかやす 　 あきら 、 1 9 9 0 年 〈 平成 2 年 〉 2 月...</td>\n",
       "      <td>4</td>\n",
       "      <td>高安晃</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>771</th>\n",
       "      <td>護廷 十 三 隊 ( ごていじゅう さん たい ) は 、 漫画 『 BLEACH 』 に ...</td>\n",
       "      <td>2</td>\n",
       "      <td>護廷十三隊</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>772 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     0  labels article\n",
       "0    よう こそ ウィキペディア 　 - 　 ウィキペディア 日本 語 版 　 - 　 百 科 事...       1  メインページ\n",
       "1    星野 　 源 ( ほしの 　 みな もと 、 1 9 8 1 年 1 月 2 8 日 　 -...       1     星野源\n",
       "2    真田 　 信繁 ( さなだ 　 のぶしげ ) は 、 安土 桃山 時代 から 江戸 時代 初...       2    真田信繁\n",
       "3    高橋 　 一生 ( たか は し 　 いっせい 、 英字 表記 : Issey 　 Taka...       2    高橋一生\n",
       "4    『 君 の 名 は 』 ( きみ の な は 、 英 : 　 Your 　 Name ) は...       1   君の名は。\n",
       "..                                                 ...     ...     ...\n",
       "767  VX ガス ( ヴィーエックス ・ ガス 、 V X 、 O - エチル - S - ( 2...       1    VXガス\n",
       "768  上杉 　 景勝 ( うえ すぎ 　 かげかつ ) は 、 戦国 時代 から 江戸 時代 前期...       4    上杉景勝\n",
       "769  有賀 　 さつき ( ありが 　 さつき 、 1 9 6 5 年 9 月 9 日 　 - 　...       4   有賀さつき\n",
       "770  髙安 　 晃 ( たかやす 　 あきら 、 1 9 9 0 年 〈 平成 2 年 〉 2 月...       4     高安晃\n",
       "771  護廷 十 三 隊 ( ごていじゅう さん たい ) は 、 漫画 『 BLEACH 』 に ...       2   護廷十三隊\n",
       "\n",
       "[772 rows x 3 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add labels and article titles to dataframe\n",
    "dfdf['labels'] = labels\n",
    "dfdf['article'] = list(wiki_df['article'].iloc[0:772])\n",
    "dfdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2a15f254",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group article text by cluster\n",
    "clustered_articles = [list(dfdf[dfdf['labels']==i][0]) for i in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "96a37f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the most frequently used words in each cluster of documents\n",
    "\n",
    "stop = ['あそこ','あっ','あの','あのかた','あの人','あり','あります','ある','あれ','い','いう','います','いる','う','うち','え','お','および','おり','おります','か','かつて','から','が','き','ここ','こちら','こと','この','これ','これら','さ','さらに','し','しかし','する','ず','せ','せる','そこ','そして','その','その他','その後','それ','それぞれ','それで','た','ただし','たち','ため','たり','だ','だっ','だれ','つ','て','で','でき','できる','です','では','でも','と','という','といった','とき','ところ','として','とともに','とも','と共に','どこ','どの','な','ない','なお','なかっ','ながら','なく','なっ','など','なに','なら','なり','なる','なん','に','において','における','について','にて','によって','により','による','に対して','に対する','に関する','の','ので','のみ','は','ば','へ','ほか','ほとんど','ほど','ます','また','または','まで','も','もの','ものの','や','よう','より','ら','られ','られる','れ','れる','を','ん','何','及び','彼','彼女','我々','特に','私','私達','貴方','貴方方''ようこそ','ウィキペディア','ウィキペディア日本語版','百科事典目次','検索資料','空白','補助記号','）', '（', '、', '『', '』', '・','：', '／', '＋', '→', '「', '」', ' ', '-', '/', '\\\\', '。', '\\n', '！', '年', '月', '日', '\\u3000', '第', '〜', '者', '人', '的', '〈', '〉', '一', '中']\n",
    "stop += list(string.punctuation)\n",
    "stop += list(string.digits)\n",
    "# had to add lots of extra stop words to eliminate frequently-used, but not meaningful terms\n",
    "\n",
    "topic_words = []\n",
    "for topic in range(5):\n",
    "    common_chars = collections.Counter()\n",
    "    for article_text in clustered_articles[topic]:\n",
    "        article_list = article_text.split(' ')\n",
    "        for word in article_list:\n",
    "            if word not in stop:\n",
    "                common_chars[word] += 1\n",
    "    topic_words.append(common_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6172a68c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common words in topic 0\n",
      "['市', '都市', '指定', '回', '加野', '後', '戦', '万', 'あさ', '屋', '日本', '大阪', '演', '銀行', '新']\n",
      "\n",
      "Most common words in topic 1\n",
      "['後', 'テレビ', '役', '話', '日本', '回', '放送', '時', '声', '登場', '版', '戦', '家', '生', '目']\n",
      "\n",
      "Most common words in topic 2\n",
      "['日本', '役', 'テレビ', '後', '版', '回', '話', '放送', '世界', '時', 'ドラマ', '声', '発売', '東京', 'よる']\n",
      "\n",
      "Most common words in topic 3\n",
      "['中国', '人民', '国', '中華', '共和', '銀行', '政府', '化', '世界', '部', '党', '経済', '問題', '億', '万']\n",
      "\n",
      "Most common words in topic 4\n",
      "['後', '役', 'テレビ', '日本', '話', '版', '回', '時', '声', 'アニメ', '際', '力', '目', '放送', '登場']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print most common words in each topic cluster of documents\n",
    "for topic in range(5):\n",
    "    print('Most common words in topic ' + str(topic))\n",
    "    print([topic_words[topic].most_common(15)[i][0] for i in range(15)])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de7a90d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc1e6fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
